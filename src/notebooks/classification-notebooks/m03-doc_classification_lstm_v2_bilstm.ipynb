{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Fixa semente aleatória para garantir que os resultados possam ser reproduzidos\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "def tokenize (text, tok):\n",
    "    return [token.text for token in tok.tokenizer(text)]\n",
    "\n",
    "def encode_sentence(text, vocab2index, tok, N=1000):\n",
    "    # tokenize text\n",
    "    tokenized = tokenize(text, tok)\n",
    "    # generate vector of size N filled with zeros\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    # get encode for each word in text, if word not in vocab2index return UNK encode = 1\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    # sentence length\n",
    "    length = min(N, len(enc1))\n",
    "    # if the sentence length is less than N, the extra spaces will be filled with zeros.\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_data(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Classe auxiliar para utilização do DataLoader\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # word_embedding, label, document length\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_data, fold):\n",
    "    X = df_data.loc[df_data['fold'] == fold, 'four_pages_encoded'].values\n",
    "    y = df_data.loc[df_data['fold'] == fold, 'label_int'].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def encode_data(df_data, config):\n",
    "    tok = spacy.load('pt_core_news_sm')\n",
    "\n",
    "    # count frequency of each word    \n",
    "    counts = Counter()\n",
    "    for index, row in df_data.loc[df_data['fold']==\"train\"].iterrows():\n",
    "        counts.update(tokenize(row['four_pages_processed'], tok))\n",
    "\n",
    "    #creating vocabulary\n",
    "    vocab2index = {\"\":0, \"UNK\":1}\n",
    "    words = [\"\", \"UNK\"]\n",
    "    for word in counts:\n",
    "        vocab2index[word] = len(words)\n",
    "        words.append(word)\n",
    "        \n",
    "    vocab_size = len(words)\n",
    "\n",
    "    # encoding\n",
    "    df_data['four_pages_encoded'] = None\n",
    "    df_data.loc[df_data['fold'] == 'train','four_pages_encoded'] = df_data.loc[df_data['fold'] == 'train','four_pages_processed'].apply(lambda x: np.array(encode_sentence(x,vocab2index, tok, config[\"num_terms\"] )))\n",
    "    df_data.loc[df_data['fold'] == 'val','four_pages_encoded'] = df_data.loc[df_data['fold'] == 'val','four_pages_processed'].apply(lambda x: np.array(encode_sentence(x,vocab2index, tok, config[\"num_terms\"] )))\n",
    "    df_data.loc[df_data['fold'] == 'test','four_pages_encoded'] = df_data.loc[df_data['fold'] == 'test','four_pages_processed'].apply(lambda x: np.array(encode_sentence(x,vocab2index, tok, config[\"num_terms\"] )))\n",
    "    \n",
    "    return df_data, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return acc, f1_macro, f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM running over word embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_embedding_dimension: int, hidden_dim: int, num_layers: int = 1, num_classes: int = 13, vocab_size: int = 0, dropout: float = 0, bidirectional: bool = True):\n",
    "        nn.Module.__init__(self)\n",
    "        self.config_keys = ['word_embedding_dimension', 'hidden_dim', 'num_layers', 'dropout', 'bidirectional']\n",
    "        self.word_embedding_dimension = word_embedding_dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embeddings_dimension = hidden_dim\n",
    "        if self.bidirectional:\n",
    "            self.embeddings_dimension *= 2\n",
    "            \n",
    "        self.embeddings = nn.Embedding(vocab_size, word_embedding_dimension, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(word_embedding_dimension, hidden_dim, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, features, sentence_length):\n",
    "        #token_embeddings = features['token_embeddings']\n",
    "        #sentence_lengths = torch.clamp(features['sentence_lengths'], min=1)\n",
    "        x = self.embeddings(features)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, sentence_length, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.encoder(packed)\n",
    "        #packed = self.encoder(packed)\n",
    "        #out_pack = nn.utils.rnn.pad_packed_sequence(packed[0], batch_first=True)\n",
    "        #out_pack = nn.utils.rnn.pad_packed_sequence(packed[0], batch_first=True)\n",
    "        #out_pack, (ht, ct) = self.encoder(out_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def get_word_embedding_dimension(self) -> int:\n",
    "        return self.embeddings_dimension\n",
    "\n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def save(self, output_path: str):\n",
    "        with open(os.path.join(output_path, 'lstm_config.json'), 'w') as fOut:\n",
    "            json.dump(self.get_config_dict(), fOut, indent=2)\n",
    "\n",
    "        torch.save(self.state_dict(), os.path.join(output_path, 'pytorch_model.bin'))\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        return {key: self.__dict__[key] for key in self.config_keys}\n",
    "\n",
    "    @staticmethod\n",
    "    def load(input_path: str):\n",
    "        with open(os.path.join(input_path, 'lstm_config.json'), 'r') as fIn:\n",
    "            config = json.load(fIn)\n",
    "\n",
    "        weights = torch.load(os.path.join(input_path, 'pytorch_model.bin'))\n",
    "        model = LSTM(**config)\n",
    "        model.load_state_dict(weights)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(config, df_data):\n",
    "    # Split data\n",
    "    X_train, y_train = split_data(df_data, \"train\")\n",
    "    X_val, y_val = split_data(df_data, \"val\")\n",
    "    X_test, y_test = split_data(df_data, \"test\")\n",
    "    \n",
    "    # Load dataset\n",
    "    train_set = load_data(X_train, y_train)\n",
    "    val_set = load_data(X_val, y_val)\n",
    "    test_set = load_data(X_test, y_test)\n",
    "    \n",
    "    data_loaders = dict()\n",
    "\n",
    "    data_loaders[\"train\"] = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    data_loaders[\"val\"] = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "        num_workers=8)\n",
    "    data_loaders[\"test\"] = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "        num_workers=8)\n",
    "    \n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_pred, y_true, fold):\n",
    "    cm = sns.heatmap(\n",
    "        confusion_matrix(y_pred, y_true, normalize=\"true\"),\n",
    "        annot=True,\n",
    "        center=0,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    plt.yticks(rotation=0)\n",
    "    img_name = \"Confusion Matrix - {}\".format(fold)\n",
    "    plt.title(img_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, checkpoint_dir=None, data_loaders=None):\n",
    "    train_loader = data_loaders[\"train\"]\n",
    "    val_loader = data_loaders[\"val\"]\n",
    "    \n",
    "    model = LSTM(\n",
    "        word_embedding_dimension = config[\"embedding_dim\"], \n",
    "        hidden_dim = config[\"embedding_dim\"], \n",
    "        num_classes = config[\"num_classes\"], \n",
    "        vocab_size = config[\"vocab_size\"],\n",
    "        num_layers = 1, \n",
    "        dropout = 0.2, \n",
    "        bidirectional = True\n",
    "    )\n",
    "        \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    model.to(device)\n",
    "    \n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_loss = float(\"inf\")\n",
    "    #best_macro = 0.0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    \"\"\"if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\"\"\"\n",
    "        \n",
    "    for epoch in range(config[\"num_epochs\"]):  # loop over the dataset multiple times\n",
    "        print(\"=\"*20,\"Epoch: {}\".format(epoch+1), \"=\"*20)\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels, sentence_length = data\n",
    "            inputs, labels = inputs.long().to(device), labels.long().to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs, sentence_length)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            epoch_steps += 1\n",
    "            training_loss += loss.cpu().detach().numpy()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_pred.extend(predicted.cpu().tolist())\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            \n",
    "        train_metrics = [(training_loss/epoch_steps)]\n",
    "        train_metrics.extend(calculate_metrics(y_true, y_pred))\n",
    "        \n",
    "        print(\"Train:\\n loss %.3f, accuracy %.3f, F1-Macro %.3f, F1-Weighted %.3f\" % (\n",
    "        train_metrics[0], train_metrics[1], train_metrics[2], train_metrics[3]))\n",
    "\n",
    "        val_metrics = eval_model(model, val_loader, device=device)\n",
    "        print(\"Val:\\n loss %.3f, accuracy %.3f, F1-Macro %.3f, F1-Weighted %.3f \\n\" % (\n",
    "        val_metrics[0], val_metrics[1], val_metrics[2], val_metrics[3]))\n",
    "        \n",
    "        #if val_metrics[2] > best_macro:\n",
    "        if val_metrics[0] < best_loss:\n",
    "            best_loss = val_metrics[0]\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_macro = val_metrics[2]\n",
    "            path = os.path.join(checkpoint_dir, \"model_v2_2.pth\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "        elif val_metrics[0] == np.nan:\n",
    "            val_metrics[0] = 10\n",
    "    print(\"Finished Training\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loader, fold=None, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    steps = 0\n",
    "    sum_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels, sentence_length = data\n",
    "            inputs, labels = inputs.long().to(device), labels.long().to(device)\n",
    "            outputs = model(inputs, sentence_length)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            steps += 1\n",
    "            sum_loss += loss.cpu().detach().numpy()\n",
    "            \n",
    "            y_pred.extend(predicted.cpu().tolist())\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "    \n",
    "    metrics = [(sum_loss/steps)]\n",
    "    metrics.extend(calculate_metrics(y_true, y_pred))\n",
    "    if fold:\n",
    "        plot_confusion_matrix(y_pred, y_true, fold)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dados01/workspace/ufmg.f01dcc/py37/lib/python3.7/site-packages/ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/dados01/workspace/ufmg.f01dcc/py37/lib/python3.7/site-packages/ipykernel_launcher.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/dados01/workspace/ufmg.f01dcc/py37/lib/python3.7/site-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/dados01/workspace/ufmg.f01dcc/py37/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch: 1 ====================\n",
      "Train:\n",
      " loss 0.514, accuracy 0.888, F1-Macro 0.286, F1-Weighted 0.894\n",
      "Val:\n",
      " loss 1.451, accuracy 0.616, F1-Macro 0.277, F1-Weighted 0.533 \n",
      "\n",
      "==================== Epoch: 2 ====================\n",
      "Train:\n",
      " loss 0.075, accuracy 0.981, F1-Macro 0.973, F1-Weighted 0.981\n",
      "Val:\n",
      " loss 1.711, accuracy 0.603, F1-Macro 0.247, F1-Weighted 0.504 \n",
      "\n",
      "==================== Epoch: 3 ====================\n",
      "Train:\n",
      " loss 0.045, accuracy 0.988, F1-Macro 0.979, F1-Weighted 0.988\n",
      "Val:\n",
      " loss 2.099, accuracy 0.633, F1-Macro 0.251, F1-Weighted 0.518 \n",
      "\n",
      "==================== Epoch: 4 ====================\n",
      "Train:\n",
      " loss 0.031, accuracy 0.991, F1-Macro 0.980, F1-Weighted 0.990\n",
      "Val:\n",
      " loss 1.945, accuracy 0.617, F1-Macro 0.237, F1-Weighted 0.521 \n",
      "\n",
      "==================== Epoch: 5 ====================\n",
      "Train:\n",
      " loss 0.027, accuracy 0.993, F1-Macro 0.984, F1-Weighted 0.993\n",
      "Val:\n",
      " loss 1.876, accuracy 0.616, F1-Macro 0.286, F1-Weighted 0.537 \n",
      "\n",
      "==================== Epoch: 6 ====================\n"
     ]
    }
   ],
   "source": [
    "def main(num_epochs, num_classes):\n",
    "    set_seed(SEED)\n",
    "        \n",
    "    #load_data(df_data)\n",
    "    config = {\n",
    "        \"embedding_dim\": 150,\n",
    "        \"dropout\": 0.5,\n",
    "        \"lr\": 0.0005,\n",
    "        \"num_terms\": 1000,\n",
    "        \"batch_size\": 24,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"num_classe\": None,\n",
    "        \"vocab_size\": None\n",
    "    }\n",
    "    if num_classes == 13:\n",
    "        df_data = pd.read_csv(\"./lstm_data/preprocessed_data_v2_2.csv\")\n",
    "    elif num_classes == 4:\n",
    "        df_data = pd.read_csv(\"./lstm_data/preprocessed_data_v2_2.csv\")\n",
    "        df_data['label_int'] = pd.factorize(df_data['final_meta-class'])[0]\n",
    "    else:\n",
    "        df_data = pd.read_csv(\"./lstm_data/preprocessed_data.csv\")\n",
    "    df_data, vocab_size = encode_data(df_data, config)\n",
    "    \n",
    "    config.update({\n",
    "        \"num_classes\": df_data['label'].nunique(),\n",
    "        \"vocab_size\": vocab_size\n",
    "    })\n",
    "    \n",
    "    data_loaders = data_load(config, df_data)\n",
    "    \n",
    "    best_model = train_model(config, checkpoint_dir=\"./lstm_data/models/\", data_loaders=data_loaders)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    best_model.to(device)\n",
    "    \n",
    "    print(\"=\"*20, \"BEST MODEL\", \"=\"*20)\n",
    "    train_metrics = eval_model(best_model, data_loaders[\"train\"], fold = \"train\", device=device)\n",
    "    print(\"Train:\\n loss %.3f, accuracy %.3f, F1-Macro %.3f, F1-Weighted %.3f\" % (\n",
    "        train_metrics[0], train_metrics[1], train_metrics[2], train_metrics[3]))\n",
    "    \n",
    "    val_metrics = eval_model(best_model, data_loaders[\"val\"], fold = \"Val\", device=device)\n",
    "    print(\"Val:\\n loss %.3f, accuracy %.3f, F1-Macro %.3f, F1-Weighted %.3f\" % (\n",
    "        val_metrics[0], val_metrics[1], val_metrics[2], val_metrics[3]))\n",
    "\n",
    "    test_metrics = eval_model(best_model, data_loaders[\"test\"], fold = \"Test\", device=device)\n",
    "    print(\"Test:\\n loss %.3f, accuracy %.3f, F1-Macro %.3f, F1-Weighted %.3f\" % (\n",
    "        test_metrics[0], test_metrics[1], test_metrics[2], test_metrics[3]))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_epochs=20, num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21 classes\n",
    "==================== BEST MODEL ====================<br/>\n",
    "Val:<br/>\n",
    " loss 0.127, accuracy 0.973, F1-Macro 0.936, F1-Weighted 0.972<br/>\n",
    "Test:<br/>\n",
    " loss 0.139, accuracy 0.966, F1-Macro 0.942, F1-Weighted 0.966<br/>\n",
    " \n",
    "### 13 classes\n",
    "==================== BEST MODEL ====================<br/>\n",
    "Val:<br/>\n",
    " loss 0.118, accuracy 0.970, F1-Macro 0.953, F1-Weighted 0.970<br/>\n",
    "Test:<br/>\n",
    " loss 0.169, accuracy 0.970, F1-Macro 0.955, F1-Weighted 0.970<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
