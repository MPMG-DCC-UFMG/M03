{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importa dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber as pdfpb\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import os.path as osp\n",
    "import pathlib\n",
    "import tarfile\n",
    "import bz2\n",
    "from zipfile import ZipFile\n",
    "from pyunpack import Archive\n",
    "\n",
    "\n",
    "##Classe com vários métodos de pre-processamento de texto em português criado pelo grupo F03\n",
    "import utils.preprocessing_portuguese as preprossPT\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from matplotlib import pyplot as plt, rcParams, ticker\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo do TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpeza_texto(page_text):\n",
    "    txt_process = preprossPT.TextPreProcessing()\n",
    "    \n",
    "    page_text = page_text.lower()\n",
    "    \n",
    "    page_text = txt_process.remove_pronouns(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_reduced_or_contracted_words(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_adverbs(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_special_characters(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_excessive_spaces(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_accents(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_stopwords(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_symbols_from_numbers(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_numbers(page_text)\n",
    "\n",
    "    page_text = txt_process.remove_urls(page_text)\n",
    "    \n",
    "    page_text = txt_process.remove_person_names(page_text)\n",
    "    \n",
    "    #Removendo letras sozinhas no texto\n",
    "    page_text = re.sub(r'(?:^| )\\w(?:$| )', ' ', page_text).strip()\n",
    "\n",
    "    page_text = page_text.replace(\"_\",\"\")\n",
    "    \n",
    "    return page_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recupera o texto de todos os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Municipio:  pirapetinga.json\n",
      "Municipio:  cristais.json\n",
      "Municipio:  coqueiral.json\n",
      "Municipio:  olaria.json\n",
      "Municipio:  arantina.json\n",
      "Municipio:  passa-vinte.json\n",
      "Municipio:  ijaci.json\n",
      "Municipio:  sao-bento-abade.json\n",
      "Municipio:  itamarati2017.json\n",
      "Municipio:  itamarati2018.json\n",
      "Municipio:  itamarati2019.json\n",
      "Municipio:  itamarati2020.json\n",
      "Municipio:  ribeirao-vermelho.json\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../data/documentos_municipios/docs-cidades-jul2021/\"\n",
    "\n",
    "#filename = f\"{base_path}documentos_json/pirapetinga.json\"\n",
    "\n",
    "files = os.listdir(base_path+\"documentos_json/\")\n",
    "\n",
    "\n",
    "\n",
    "cities = [city.split(\".\")[0] for city in files if city.split(\".\")[1]=='json']\n",
    "\n",
    "docs_content = []\n",
    "docs_pages = []\n",
    "#print(files)\n",
    "\n",
    "\n",
    "for filename in files:\n",
    "    if os.path.splitext(filename)[1] == '.json':\n",
    "        print(\"Municipio: \", filename)\n",
    "        with open(base_path+\"documentos_json/\"+filename) as f:\n",
    "            lines = f.readlines() # lê o conteúdo (pode ser lido em um stream, se achar necessário)\n",
    "            #print(\"document id                     \", \"  #\", \" status\")\n",
    "            for line in lines: # um doc por linha\n",
    "                document = json.loads(line) # alguns arquivos tem problemas, portanto, verifique o status.\n",
    "                #print(document[\"file_id\"], f\"{document['n_pages']:>03d}\", document[\"status\"])\n",
    "                if document[\"status\"] != \"FAILED\" and document[\"text_content\"]:\n",
    "                    #print(document[\"file_id\"])\n",
    "                    #print(document[\"text_content\"][0:1])\n",
    "                    #Gera texto do documento com as 4 páginas                    \n",
    "                    corpus = [''.join(page) for page in document[\"text_content\"][:4]]\n",
    "                    docs_content.append(corpus) \n",
    "                    docs_pages.append(document[\"n_pages\"]) \n",
    "\n",
    "#print(len(corpus))\n",
    "#print(len(docs_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_texts = docs_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f0464ea79e46fea5812060f553ee83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5876.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#print(unprocessed_texts[:5])\n",
    "for i in tqdm(range(len(unprocessed_texts))):\n",
    "    for city in cities:\n",
    "        docs_content[i] = [doc.lower().replace(city, \"\") for doc in unprocessed_texts[i]]\n",
    "\n",
    "    docs_content[i] = [limpeza_texto(doc) for doc in docs_content[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcular TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8271e368b554a3c9932e592f62e1c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5876.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "frequency = []\n",
    "feature_names = []\n",
    "path_save_output = \"../data/caracterizacao/Entrada-classificador/\"\n",
    "count_doc = 0\n",
    "\n",
    "results.setdefault(\"num_page\", [])\n",
    "\n",
    "#print(docs_content[0])\n",
    "\n",
    "#Ler 4 páginas\n",
    "#for num_pages in range(4):\n",
    "\n",
    "for i in tqdm(range(len(docs_content))):\n",
    "    #print(doc)\n",
    "    try:\n",
    "        vectorizer = CountVectorizer()\n",
    "        # calcula tf\n",
    "        #corpus = [' '.join(page[:num_pages+1]) for page in pages_content]\n",
    "        frequency = vectorizer.fit_transform(docs_content[i])\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        inv_doc_freq = tfidf_transformer.fit(frequency)\n",
    "\n",
    "        df = pd.DataFrame(columns = feature_names)\n",
    "        df.loc[len(df)] = np.squeeze(np.asarray(frequency.sum(axis=0)))\n",
    "        df.loc[len(df)] = inv_doc_freq.idf_\n",
    "        results[count_doc] = df\n",
    "        results[\"num_page\"].append(docs_pages[i]) \n",
    "        count_doc += 1\n",
    "    except:\n",
    "        print(docs_content[i])\n",
    "        pass\n",
    "    \n",
    "    \n",
    "#print(results.keys())\n",
    "    # top k  frequency\n",
    "    #print(df.T.sort_values(by=0, ascending=False).head(10).T)\n",
    "\n",
    "    # top k idf\n",
    "    #df.T.sort_values(by=1, ascending=False).head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar tf-idf para todos os docmuentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_save_output+\"tf_1_to_4pages.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'tf_1_to_4pages.json')\n",
    "    \n",
    "if not os.path.exists(path_save_output+\"idf_1_to_4pages.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'idf_1_to_4pages.json')\n",
    "\n",
    "\n",
    "filename_tf = path_save_output+\"tf_1_to_4pages.json\"\n",
    "filename_idf = path_save_output+'idf_1_to_4pages.json'\n",
    "\n",
    "with open(filename_tf, \"wt+\", encoding=\"utf-8\") as f:\n",
    "    for key in tqdm(results.keys()):\n",
    "        \n",
    "        \n",
    "\n",
    "        if key != 'num_page': \n",
    "            #print(key)\n",
    "            #print(len(results))\n",
    "            #print(results[key][0])\n",
    "\n",
    "\n",
    "            #df_tf = pd.DataFrame(columns=[\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)])\n",
    "            #df_idf = pd.DataFrame(columns=[\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_idf\".format(key)])\n",
    "\n",
    "            df = results[key].astype(float)\n",
    "            #print(df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1))\n",
    "            df_tf = df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1)\n",
    "            df_tf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "            df_tf[\"num_page\"] = results[\"num_page\"][key]\n",
    "\n",
    "            df_tf = df_tf.to_json()\n",
    "            f.write(df_tf)\n",
    "\n",
    "            \n",
    "            '''print(\"Add column pages!\")\n",
    "\n",
    "            df_tf = pd.DataFrame(columns=[\"num_page\"])\n",
    "            \n",
    "            df_tf[\"num_page\"] = results[\"num_page\"]\n",
    "            \n",
    "            df_tf = df_tf.to_json()\n",
    "            f.write(df_tf)'''\n",
    "            \n",
    "            \n",
    "with open(filename_idf, \"wt+\", encoding=\"utf-8\") as f:\n",
    "    for key in tqdm(results.keys()):\n",
    "\n",
    "        if key != 'num_page': \n",
    "\n",
    "\n",
    "            df = results[key].astype(float)\n",
    "\n",
    "            df_idf = df.T.sort_values(by=1,ascending=False).reset_index().reset_index().drop(0, axis=1)\n",
    "\n",
    "            df_idf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "            \n",
    "            df_idf[\"num_page\"] = results[\"num_page\"][key]\n",
    "\n",
    "            df_idf = df_idf.to_json()\n",
    "            f.write(df_idf)\n",
    "\n",
    "        '''else:\n",
    "            print(\"Add column pages!\")\n",
    "\n",
    "            df_idf = pd.DataFrame(columns=[\"num_page\"])\n",
    "\n",
    "            df_idf[\"num_page\"] = results[\"num_page\"]\n",
    "\n",
    "            df_idf = df_idf.to_json()\n",
    "            f.write(df_idf)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar TF-IDF apenas das palavras-chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords = [\"ata\", \"sessão\", \"pública\", \"homologacao\", \"adjudicacao\", \"convite\", \"edital\", \"cronograma\", \"aditamento\", \"retificacao\", \"contrato\", \"administrativo\", \"ordem\", \"servico\", \n",
    "            \"resposta\", \"extrato\", \"diário\", \"oficial\", \"aviso\"]\n",
    "\n",
    "\n",
    "#print(keywords_of_interest)\n",
    "\n",
    "if not os.path.exists(path_save_output+\"tf_1_to_4pages_palavras-chave.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'tf_1_to_4pages_palavras-chave.json')\n",
    "    \n",
    "if not os.path.exists(path_save_output+\"idf_1_to_4pages_palavras-chave.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'idf_1_to_4pages_palavras-chave.json')\n",
    "\n",
    "\n",
    "filename_tf = path_save_output+\"tf_1_to_4pages_palavras-chave.json\"\n",
    "filename_idf = path_save_output+'idf_1_to_4pages_palavras-chave.json'\n",
    "\n",
    "with open(filename_tf, \"wt+\", encoding=\"utf-8\") as f:\n",
    "    for key in tqdm(results.keys()):\n",
    "        \n",
    "        if key != 'num_page': \n",
    "\n",
    "            df = results[key].astype(float)\n",
    "            df = df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1)\n",
    "            #print(df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1))\n",
    "            df_tf = df.loc[df[\"index\"].isin(keywords)]\n",
    "            #print(df_tf)\n",
    "            df_tf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "            df_tf[\"num_page\"] = results[\"num_page\"][key]\n",
    "\n",
    "            df_tf = df_tf.to_json()\n",
    "            f.write(df_tf)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "with open(filename_idf, \"wt+\", encoding=\"utf-8\") as f:\n",
    "    for key in tqdm(results.keys()):\n",
    "\n",
    "        if key != 'num_page': \n",
    "\n",
    "\n",
    "            df = results[key].astype(float)\n",
    "           \n",
    "\n",
    "            df = df.T.sort_values(by=1,ascending=False).reset_index().reset_index().drop(0, axis=1)\n",
    "            df_idf = df.loc[df[\"index\"].isin(keywords)]\n",
    "\n",
    "            df_idf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "            \n",
    "            df_idf[\"num_page\"] = results[\"num_page\"][key]\n",
    "\n",
    "            df_idf = df_idf.to_json()\n",
    "            f.write(df_idf)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb740d2b108749e1a6621781312397a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5876.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "frequency = []\n",
    "feature_names = []\n",
    "count_doc = 0\n",
    "\n",
    "results.setdefault(\"num_page\", [])\n",
    "\n",
    "#print(docs_content[0])\n",
    "\n",
    "#Ler 4 páginas\n",
    "#for num_pages in range(4):\n",
    "\n",
    "for i in tqdm(range(len(docs_content))):\n",
    "    #print(doc)\n",
    "    try:\n",
    "        vectorizer = CountVectorizer(ngram_range=(1,1)) # to use bigrams ngram_range=(2,2)\n",
    "\n",
    "        #transform\n",
    "        frequency = vectorizer.fit_transform(docs_content[i])\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(columns = feature_names)\n",
    "        df.loc[len(df)] = np.squeeze(np.asarray(frequency.sum(axis=0)))\n",
    "\n",
    "        results[count_doc] = df\n",
    "        results[\"num_page\"].append(docs_pages[i]) \n",
    "        count_doc += 1\n",
    "    except:\n",
    "        print(docs_content[i])\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2872d7a1b3441ead91483223172400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5875.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dados01/workspace/ufmg.f01dcc/py37/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"ata\", \"sessão\", \"pública\", \"homologacao\", \"adjudicacao\", \"convite\", \"edital\", \"cronograma\", \"aditamento\", \"retificacao\", \"contrato\", \"administrativo\", \"ordem\", \"servico\", \n",
    "            \"resposta\", \"extrato\", \"diário\", \"oficial\", \"aviso\"]\n",
    "path_save_output = \"../data/caracterizacao/Entrada-classificador/\"\n",
    "\n",
    "#print(keywords_of_interest)\n",
    "\n",
    "if not os.path.exists(path_save_output+\"BoW_1_to_4pages_palavras-chave.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'BoW_1_to_4pages_palavras-chave.json')\n",
    "    \n",
    "\n",
    "filename = path_save_output+\"BoW_1_to_4pages_palavras-chave.json\"\n",
    "\n",
    "\n",
    "with open(filename, \"wt+\", encoding=\"utf-8\") as f:\n",
    "    for key in tqdm(results.keys()):\n",
    "        \n",
    "        if key != 'num_page': \n",
    "\n",
    "            df = results[key].astype(float)\n",
    "            df = df.T.sort_values(by=0,ascending=False).reset_index().reset_index()\n",
    "            df_BoW = df.loc[df[\"index\"].isin(keywords)]\n",
    "            #print(df_BoW)\n",
    "            #print(df_tf)\n",
    "            df_BoW.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_freq\".format(key)]\n",
    "            df_BoW[\"num_page\"] = results[\"num_page\"][key]\n",
    "\n",
    "            df_BoW = df_BoW.to_json()\n",
    "            f.write(df_BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume muita memória salvar em csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_save_output+\"tf_1_to_4pages.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'tf_1_to_4pages.json')\n",
    "    \n",
    "if not os.path.exists(path_save_output+\"idf_1_to_4pages.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'idf_1_to_4pages.json')\n",
    "\n",
    "\n",
    "filename_tf = path_save_output+\"tf_1_to_4pages.json\"\n",
    "filename_idf = path_save_output+'idf_1_to_4pages.json'\n",
    "\n",
    "with open(filename_tf, \"wt+\", encoding=\"utf-8\") as f:\n",
    "    for key in tqdm(results.keys()):\n",
    "        \n",
    "        \n",
    "\n",
    "        if key != 'num_page': \n",
    "            print(key)\n",
    "            print(len(results))\n",
    "            print(results[key][0])\n",
    "\n",
    "\n",
    "            #df_tf = pd.DataFrame(columns=[\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)])\n",
    "            #df_idf = pd.DataFrame(columns=[\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_idf\".format(key)])\n",
    "\n",
    "            df = results[key].astype(float)\n",
    "            #print(df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1))\n",
    "            df_tf = df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1)\n",
    "            df_tf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "\n",
    "\n",
    "            df_tf = df_tf.to_json()\n",
    "            f.write(df_tf)\n",
    "\n",
    "        else:\n",
    "            print(\"Add column pages!\")\n",
    "\n",
    "            df_tf = pd.DataFrame(columns=[\"num_page\"])\n",
    "            \n",
    "            df_tf[\"num_page\"] = results[\"num_page\"]\n",
    "            \n",
    "            df_tf = df_tf.to_json()\n",
    "            f.write(df_tf)\n",
    "            \n",
    "            \n",
    "with open(filename_idf, \"wt+\", encoding=\"utf-8\") as f:\n",
    "    for key in tqdm(results.keys()):\n",
    "\n",
    "        if key != 'num_page': \n",
    "\n",
    "\n",
    "            df = results[key].astype(float)\n",
    "\n",
    "            df_idf = df.T.sort_values(by=1,ascending=False).reset_index().reset_index().drop(0, axis=1)\n",
    "\n",
    "            df_idf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "\n",
    "            df_idf = df_idf.to_json()\n",
    "            f.write(df_idf)\n",
    "\n",
    "        else:\n",
    "            print(\"Add column pages!\")\n",
    "\n",
    "            df_idf = pd.DataFrame(columns=[\"num_page\"])\n",
    "\n",
    "            df_idf[\"num_page\"] = results[\"num_page\"]\n",
    "\n",
    "            df_idf = df_idf.to_json()\n",
    "            f.write(df_idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_save_output+\"tf_1_to_4pages.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'tf_1_to_4pages.json')\n",
    "    \n",
    "if not os.path.exists(path_save_output+\"idf_1_to_4pages.json\"):\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(path_save_output+'idf_1_to_4pages.json')\n",
    "\n",
    "for key in tqdm(results.keys()):\n",
    "    \n",
    "    df_tf = pd.read_csv(path_save_output+\"tf_1_to_4pages.json\")\n",
    "    df_idf = pd.read_csv(path_save_output+\"idf_1_to_4pages.json\")\n",
    "    \n",
    "    if key != 'num_page': \n",
    "        \n",
    "        #df_tf = pd.DataFrame(columns=[\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)])\n",
    "        #df_idf = pd.DataFrame(columns=[\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_idf\".format(key)])\n",
    "\n",
    "        df = results[key].astype(float)\n",
    "        #print(df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1))\n",
    "        df_aux = df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1)\n",
    "        #print(df_tf[\"level_0\"])\n",
    "        \n",
    "        df_tf[\"{}_index_doc\".format(key)] = df_aux[\"level_0\"]\n",
    "        df_tf[\"{}_term\".format(key)] = df_aux[\"index\"]\n",
    "        df_tf[\"{}_tf\".format(key)] = df_aux[0]\n",
    "        \n",
    "        #df_tf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "\n",
    "        df_aux = df.T.sort_values(by=1,ascending=False).reset_index().reset_index().drop(0, axis=1)\n",
    "        \n",
    "        \n",
    "        df_idf[\"{}_index_doc\".format(key)] = df_aux[\"level_0\"]\n",
    "        df_idf[\"{}_term\".format(key)] = df_aux[\"index\"]\n",
    "        df_idf[\"{}_idf\".format(key)] = df_aux[1]\n",
    "        \n",
    "        #df_idf.columns = [\"{}_index_doc\".format(key),\"{}_term\".format(key), \"{}_tf\".format(key)]\n",
    "        \n",
    "        #df_tf_all = pd.concat([df_tf_all, df_tf], axis=1)\n",
    "        #df_idf_all = pd.concat([df_idf_all, df_idf], axis=1)\n",
    "        #print(df_tf_all)\n",
    "        \n",
    "        df_tf.to_json(path_save_output+\"tf_1_to_4pages.csv\", index=False)\n",
    "\n",
    "        df_idf.to_json(path_save_output+\"idf_1_to_4pages.csv\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"Add column pages!\")\n",
    "        \n",
    "        df_tf = pd.DataFrame(columns=[\"num_page\"])\n",
    "        df_idf = pd.DataFrame(columns=[\"num_page\"])\n",
    "        \n",
    "        #df_tf.columns = [\"{}_index\".format(prefix),\"{}_term\".format(prefix), \"{}_tf\".format(prefix), \"num_page\"]\n",
    "        df_tf[\"num_page\"] = results[\"num_page\"]\n",
    "        \n",
    "        #df_idf.columns = [\"{}_index\".format(prefix),\"{}_term\".format(prefix), \"{}_idf\".format(prefix), \"num_page\"]\n",
    "        df_idf[\"num_page\"] = results[\"num_page\"]\n",
    "        \n",
    "        df_tf.to_json(path_save_output+\"tf_1_to_4pages.csv\", index=False)\n",
    "\n",
    "        df_idf.to_json(path_save_output+\"idf_1_to_4pages.csv\", index=False)\n",
    "        \n",
    "        \n",
    "#print(df_tf_all)\n",
    "\n",
    "#df_tf_all.to_csv(path_save_output+\"tf_1_to_4pages.csv\")\n",
    "\n",
    "#df_idf_all.to_csv(path_save_output+\"idf_1_to_4pages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tamanho do vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = []\n",
    "for key in results.keys():\n",
    "    vocab_size.append(results[key].shape[1])\n",
    "plt.bar(range(1,12),vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(results.keys()):\n",
    "    '''if key == 1001:\n",
    "        prefix = \"{}_doc\".format(\"All\")\n",
    "    else:'''\n",
    "    if key != 'num_page':\n",
    "        prefix = \"{}_doc\".format(key)        \n",
    "\n",
    "        df = results[key].astype(float)\n",
    "        df_tf = df.T.sort_values(by=0,ascending=False).reset_index().reset_index().drop(1, axis=1)\n",
    "        df_tf.columns = [\"{}_index\".format(prefix),\"{}_term\".format(prefix), \"{}_tf\".format(prefix), \"{}_num_page\".format(prefix)]\n",
    "\n",
    "        df_idf = df.T.sort_values(by=1,ascending=False).reset_index().reset_index().drop(0, axis=1)\n",
    "        df_idf.columns = [\"{}_index\".format(prefix),\"{}_term\".format(prefix), \"{}_idf\".format(prefix), \"{}_num_page\".format(prefix)]\n",
    "        if key == 1:\n",
    "            df_tf_all = df_tf.copy()\n",
    "            df_idf_all = df_idf.copy()\n",
    "        else:\n",
    "            df_tf_all = pd.concat([df_tf_all, df_tf], axis=1)\n",
    "            df_idf_all = pd.concat([df_idf_all, df_idf], axis=1)\n",
    "\n",
    "    else:\n",
    "        print(\"Add collumn pages!\")\n",
    "        df_tf.columns = [\"{}_index\".format(prefix),\"{}_term\".format(prefix), \"{}_tf\".format(prefix), \"{}_num_page\".format(prefix)]\n",
    "        df[\"{}_num_page\".format(prefix)] = results[\"num_page\"]\n",
    "        \n",
    "        \n",
    "print(df_tf_all)\n",
    "\n",
    "df_tf_all.to_csv(path_save_output+\"tf_1_to_4pages.csv\")\n",
    "\n",
    "df_idf_all.to_csv(path_save_output+\"idf_1_to_4pages.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 20,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_terms_in_graph(df, terms, tf, pages_to_plot):\n",
    "    scatters = []\n",
    "    #term_columns = [col for col in df.columns if \"term\" in col]\n",
    "    suffix = 'term'\n",
    "    if pages_to_plot == \"All\":\n",
    "        term_columns = [col for col in df.columns if suffix in col]\n",
    "    else:\n",
    "        list_of_pages = [str(i+1) for i in range(pages_to_plot)]\n",
    "        term_columns = [col for col in df.columns if suffix in col and col.split(\"_\")[0] in list_of_pages]\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(terms)))\n",
    "    #colors = ['b', 'g', 'r', 'c', 'k', 'm', 'y']\n",
    "    for i, term in enumerate(terms):\n",
    "        points = []\n",
    "        for col in term_columns:\n",
    "            num_pages = col.split(\"_\")[0]\n",
    "            try:\n",
    "                if tf:\n",
    "                    points.append(df.loc[df[col] == term, [\"{}_page_index\".format(num_pages), \"{}_page_tf\".format(num_pages)]].values[0])\n",
    "                else:\n",
    "                    points.append(df.loc[df[col] == term, [\"{}_page_index\".format(num_pages), \"{}_page_idf\".format(num_pages)]].values[0])\n",
    "            except:\n",
    "                print(term)\n",
    "                break\n",
    "        x_axis = [point[0] for point in points]\n",
    "        y_axis = [point[1] for point in points]\n",
    "        if i%2 == 0:\n",
    "            marker = \"x\"\n",
    "        else:\n",
    "            marker = \"o\"\n",
    "        scatters.append(plt.scatter(x=x_axis, y=y_axis,zorder=100, color=colors[i], label=term, marker=marker))\n",
    "        #plt.yscale('log')\n",
    "    \n",
    "    \"\"\"plt.legend(scatters,\n",
    "                   terms,\n",
    "                   scatterpoints=1,\n",
    "                   loc='upper right',\n",
    "                   ncol=3,\n",
    "                   fontsize=8)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms =  ['convite','ata', 'edital', 'adjudicacao', 'homologacao', 'errata', 'anexo', 'aditivo', 'habilitacao', 'julgamento', 'designacao', 'aviso', 'contrato',  'deliberacao', 'revogacao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(df, terms, tf, terms_filter = None, pages_to_plot = \"All\", figsize=(10.4,7)):\n",
    "    if tf:\n",
    "        suffix = 'tf'\n",
    "    else:\n",
    "        suffix = 'idf'\n",
    "    if pages_to_plot == \"All\":\n",
    "        cols_to_plot = [col for col in df.columns if suffix in col]\n",
    "    else:\n",
    "        list_of_pages = [str(i+1) for i in range(pages_to_plot)]\n",
    "        cols_to_plot = [col for col in df.columns if suffix in col and col.split(\"_\")[0] in list_of_pages]\n",
    "    plt.figure(figsize=figsize)\n",
    "    for col in cols_to_plot:\n",
    "        num_pages = col.split(\"_\")[0]\n",
    "        #points = df[[\"{}_page_index\".format(num_pages), \"{}_page_{}\".format(num_pages, suffix)]].values#[10000:]\n",
    "        if terms_filter != None:            \n",
    "            points = df.loc[df[\"{}_page_term\".format(num_pages)].isin(terms_filter[num_pages]), [\"{}_page_index\".format(num_pages), \"{}_page_{}\".format(num_pages, suffix)]].values#[10000:]\n",
    "        else:\n",
    "            points = df[[\"{}_page_index\".format(num_pages), \"{}_page_{}\".format(num_pages, suffix)]].values#[10000:]\n",
    "        if num_pages == str(1):\n",
    "            plt.plot(points[:,0], points[:,1], label=\"{}_pag\".format(num_pages), zorder = 80)\n",
    "        elif num_pages == \"All\":\n",
    "            plt.plot(points[:,0], points[:,1], label=\"Todas_pags\", zorder = 80)\n",
    "        else:\n",
    "            plt.plot(points[:,0], points[:,1], label=\"1-{}_pags\".format(num_pages), zorder = 80)\n",
    "\n",
    "    insert_terms_in_graph(df, terms, tf, pages_to_plot)\n",
    "    \n",
    "    plt.grid(zorder=1)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.legend(ncol=6,bbox_to_anchor=(0, -0.35), loc='lower left', borderaxespad=0.)\n",
    "    if tf:\n",
    "        #plt.yscale('log')\n",
    "        #plt.legend( ncol=3, loc='upper right')\n",
    "        plt.ylabel(\"Frequência dos Termos\")\n",
    "    else:\n",
    "        #plt.legend(ncol=6,bbox_to_anchor=(0, -0.27), loc='lower left', borderaxespad=0.)\n",
    "        plt.ylabel(\"Frequência Inversa dos Documentos\")\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_vocab_size(vocab_dict, figsize=(10,6)):\n",
    "    x_label = []\n",
    "    y_axis = []\n",
    "    for key in vocab_dict.keys():\n",
    "        y_axis.append(len(vocab_dict[key]))\n",
    "        x_label.append(key)\n",
    "    plt.figure(figsize=figsize)\n",
    "    splot = sns.barplot(x= x_label, y=y_axis, color='#66c3a6', zorder=100)\n",
    "    for p in splot.patches:\n",
    "        splot.annotate(format(p.get_height(), '.0f'), \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha = 'center', va = 'center', \n",
    "                       xytext = (0, 9), \n",
    "                       textcoords = 'offset points')\n",
    "    plt.grid(zorder=1)\n",
    "    plt.xlabel(\"Número de Páginas\")\n",
    "    plt.ylabel(\"Tamanho do Vocabulário\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(df, terms, tf, terms_filter = None, pages_to_plot = \"All\", figsize=(10.4,7)):\n",
    "    if tf:\n",
    "        suffix = 'tf'\n",
    "    else:\n",
    "        suffix = 'idf'\n",
    "    if pages_to_plot == \"All\":\n",
    "        cols_to_plot = [col for col in df.columns if suffix in col]\n",
    "    else:\n",
    "        list_of_pages = [str(i+1) for i in range(pages_to_plot)]\n",
    "        cols_to_plot = [col for col in df.columns if suffix in col and col.split(\"_\")[0] in list_of_pages]\n",
    "    #plt.figure(figsize=figsize)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for col in cols_to_plot:\n",
    "        num_pages = col.split(\"_\")[0]\n",
    "        #points = df[[\"{}_page_index\".format(num_pages), \"{}_page_{}\".format(num_pages, suffix)]].values#[10000:]\n",
    "        if terms_filter != None:            \n",
    "            points = df.loc[df[\"{}_page_term\".format(num_pages)].isin(terms_filter[num_pages]), [\"{}_page_index\".format(num_pages), \"{}_page_{}\".format(num_pages, suffix)]].values#[10000:]\n",
    "        else:\n",
    "            points = df[[\"{}_page_index\".format(num_pages), \"{}_page_{}\".format(num_pages, suffix)]].values#[10000:]\n",
    "        if num_pages == str(1):\n",
    "            ax.plot(points[:,0], points[:,1], label=\"{}_pag\".format(num_pages), zorder = 80)\n",
    "        elif num_pages == \"All\":\n",
    "            ax.plot(points[:,0], points[:,1], label=\"Todas_pags\", zorder = 80)\n",
    "            max_x = max(points[:,0])\n",
    "        else:\n",
    "            ax.plot(points[:,0], points[:,1], label=\"1-{}_pags\".format(num_pages), zorder = 80)\n",
    "\n",
    "    insert_terms_in_graph(df, terms, tf, pages_to_plot)\n",
    "    \n",
    "    ax.grid(zorder=1)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "\n",
    "    if tf:\n",
    "        #plt.yscale('log')\n",
    "        #plt.legend( ncol=3, loc='upper right')\n",
    "        plt.ylabel(\"Frequência dos Termos\")\n",
    "    else:\n",
    "        #ax.set_yticklabels()\n",
    "        #plt.legend(ncol=6,bbox_to_anchor=(0, -0.27), loc='lower left', borderaxespad=0.)\n",
    "        if num_pages == \"All\":\n",
    "            plt.plot( [0,max_x],[8,8], color='k', linestyle='dashed')\n",
    "        plt.ylabel(\"Frequência Inversa dos Documentos\")\n",
    "        #plt.yticks([x for x in range(1,11)])        \n",
    "        ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "        #ax.set_yticklabels([str(x) for x in range(1,11)]) \n",
    "    plt.legend(ncol=6,bbox_to_anchor=(0, -0.35), loc='lower left', borderaxespad=0.)\n",
    "    plt.xlabel(\"Rank (Tamanho do Vocabulário)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_vocab_size(vocab_dict, figsize=(10.4,7)):\n",
    "    x_label = []\n",
    "    y_axis = []\n",
    "    for key in vocab_dict.keys():\n",
    "        y_axis.append(len(vocab_dict[key][0]))\n",
    "        x_label.append(key)\n",
    "    plt.figure(figsize=figsize)\n",
    "    splot = sns.barplot(x= x_label, y=y_axis, color='#66c3a6', zorder=100)\n",
    "    \n",
    "    x_label = []\n",
    "    y_axis = []    \n",
    "    for key in vocab_dict.keys():\n",
    "        y_axis.append(len(vocab_dict[key][1]))\n",
    "        x_label.append(key)\n",
    "    #plt.figure(figsize=figsize)\n",
    "    splot2 = sns.barplot(x= x_label, y=y_axis, color='#c3ac66',  zorder=101)\n",
    "    for p in splot.patches:\n",
    "        splot.annotate(format(p.get_height(), '.0f'), \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha = 'center', va = 'center', \n",
    "                       xytext = (0, 9), \n",
    "                       textcoords = 'offset points')\n",
    "    for p in splot2.patches:\n",
    "        splot2.annotate(format(p.get_height(), '.0f'), \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha = 'center', va = 'center', \n",
    "                       xytext = (0, 9), \n",
    "                       textcoords = 'offset points', zorder=102)\n",
    "    plt.legend([\"Todos termos\", \"Termos com IDF < 8\"])\n",
    "    ax = plt.gca()\n",
    "    leg = ax.get_legend()\n",
    "    leg.legendHandles[0].set_color('#66c3a6')\n",
    "    leg.legendHandles[1].set_color('#c3ac66')\n",
    "    plt.grid(zorder=1)\n",
    "    plt.xlabel(\"Número de Páginas\")\n",
    "    plt.ylabel(\"Tamanho do Vocabulário\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todas as páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lines(df_tf_all, terms, tf = True, pages_to_plot=\"All\", figsize=(10.4,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lines(df_idf_all, terms, tf = False, pages_to_plot=\"All\", figsize=(10.4,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked\n",
    "terms_filter = {}\n",
    "for col in df_idf_all.columns:\n",
    "    if \"term\" in col:\n",
    "        key = col.split(\"_\")[0]\n",
    "        if key == \"All\":\n",
    "            key_dict = \"Todas_pags\"\n",
    "        else:\n",
    "            key_dict = key\n",
    "        terms_filter[key_dict] = [df_idf_all[col].dropna().values]\n",
    "        terms_filter[key_dict].append(df_idf_all.loc[df_idf_all[\"{}_page_idf\".format(key)] < 8, col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vocab_size(terms_filter, (10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Até 10 páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pages_to_plot = 10\n",
    "plot_lines(df_tf_all, terms, tf = True, pages_to_plot=pages_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lines(df_idf_all, terms, tf = False, pages_to_plot=pages_to_plot,  figsize=(10.4,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 primeiras páginas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "suffix = 'tf'\n",
    "terms_filter_tf = {}\n",
    "num_pages = 10\n",
    "filter_value_tf = 5\n",
    "for num_page in range(1,num_pages+1):\n",
    "    terms_filter_tf[str(num_page)] = df_tf_all.loc[df_tf_all[\"{}_page_tf\".format(num_page)] >= filter_value_tf, [\"{}_page_term\".format(num_page)]].values[:,0]\n",
    "plot_lines(df_tf_all, terms, True, terms_filter_tf, num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lines(df_idf_all, terms, False, terms_filter_tf, num_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idf < 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "suffix = 'idf'\n",
    "terms_filter_idf = {}\n",
    "num_pages = 10\n",
    "filter_value_idf = 8\n",
    "for num_page in range(1,num_pages+1):\n",
    "    terms_filter_idf[str(num_page)] = df_idf_all.loc[df_idf_all[\"{}_page_idf\".format(num_page)] < filter_value_idf, [\"{}_page_term\".format(num_page)]].values[:,0]    \n",
    "plot_lines(df_tf_all, terms, True, terms_filter_idf, num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lines(df_idf_all, terms, False, terms_filter_idf, num_pages)#,  figsize=(10.55,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vocab_size(terms_filter_idf, (10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf >=10 idf < 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "suffix = 'idf'\n",
    "terms_filter_intersection = {}\n",
    "num_pages = 10\n",
    "for num_page in range(1,num_pages+1): \n",
    "    terms_filter_intersection[str(num_page)] = set(terms_filter_idf[str(num_page)]).intersection(terms_filter_tf[str(num_page)])\n",
    "plot_lines(df_tf_all, terms, True, terms_filter_intersection, num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lines(df_idf_all, terms, False, terms_filter_intersection, num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(5876/20)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf_all.loc[df_idf_all['3_page_term'] == 'cid', '4_page_idf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuvem de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def gera_wordcloud(df, pg, metodo, nomeArq, terms, figsize=(10.4,7)):\n",
    "    \n",
    "    # import the desired colormap from matplotlib\n",
    "    cmap = mpl.cm.YlGnBu(np.linspace(0,1,20)) \n",
    "    # the darker part of the matrix is selected for readability\n",
    "    cmap = mpl.colors.ListedColormap(cmap[-10:,:-1])\n",
    "    \n",
    "    #tf baixo tende a implicar em idf alto\n",
    "    df = df.loc[df[pg+'_page_term'].isin(terms), [pg+'_page_term', pg+'_page_'+metodo]].sort_values(by=[pg+'_page_'+metodo], ascending=False)\n",
    "    #print(df.head(30))\n",
    "\n",
    "    df = df[(df[pg+\"_page_term\"] != \"minas\") & (df[pg+\"_page_term\"]  != \"mg\") & (df[pg+\"_page_term\"]  != \"gerais\")]\n",
    "    #print(freq)\n",
    "    dictTerm = dict(zip(df[pg+\"_page_term\"], df[pg+'_page_'+metodo]))\n",
    "    \n",
    "    #wordcloud = WordCloud(background_color=\"white\", max_words=150, colormap=cmap).generate_from_frequencies(dictTerm)\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=150, colormap=cmap, ).fit_words(dictTerm)\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # remove plot axes\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "\n",
    "    # save the image\n",
    "    #plt.savefig(base_path+nomeArq)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Filtrar IDF >= 8\n",
    "idf = 8\n",
    "terms_by_page_idf = []\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['1_page_idf'] >= idf, '1_page_term'].values)\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['2_page_idf'] >= idf, '2_page_term'].values)\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['3_page_idf'] >= idf, '3_page_term'].values)\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['4_page_idf'] >= idf, '4_page_term'].values)\n",
    "\n",
    "gera_wordcloud(df_tf_all, str(1), \"tf\", \"wordcloudIDFMa8_1pg.png\", terms_by_page_idf[0], figsize=(6, 4))\n",
    "\n",
    "gera_wordcloud(df_tf_all, str(2), \"tf\", \"wordcloudIDFMa8_2pg.png\", terms_by_page_idf[1], figsize=(6, 4))\n",
    "\n",
    "gera_wordcloud(df_tf_all, str(3), \"tf\", \"wordcloudIDFMa8_3pg.png\", terms_by_page_idf[2], figsize=(6, 4))\n",
    "\n",
    "gera_wordcloud(df_tf_all, str(4), \"tf\", \"wordcloudIDFMa8_3pg.png\", terms_by_page_idf[3], figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrar IDF >= 8\n",
    "idf = 8\n",
    "terms_by_page_idf = []\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['1_page_idf'] < idf, '1_page_term'].values)\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['2_page_idf'] < idf, '2_page_term'].values)\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['3_page_idf'] < idf, '3_page_term'].values)\n",
    "terms_by_page_idf.append(df_idf_all.loc[df_idf_all['4_page_idf'] < idf, '4_page_term'].values)\n",
    "\n",
    "gera_wordcloud(df_tf_all, str(1), \"tf\", \"wordcloudIDFMa8_1pg.png\", terms_by_page_idf[0], figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gera_wordcloud(df_tf_all, str(2), \"tf\", \"wordcloudIDFMa8_2pg.png\", terms_by_page_idf[1], figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gera_wordcloud(df_tf_all, str(3), \"tf\", \"wordcloudIDFMa8_3pg.png\", terms_by_page_idf[2], figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gera_wordcloud(df_tf_all, str(4), \"tf\", \"wordcloudIDFMa8_3pg.png\", terms_by_page_idf[3], figsize=(6, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
